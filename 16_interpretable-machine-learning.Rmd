# Interpretable Machine Learning

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  eval = FALSE
)
```

**Learning objectives:**

1. Understand the importance of interpretability in machine learning models.
2. Learn about global and local interpretation.
3. Understand the trade-off in interpretation.
4. Learn about model-specific implementation.
5. Understand the concept of permutation-based feature importance.
6. Understand the concept of partial dependence.
7. Learn about Individual Conditional Expectation (ICE).

## Introduction

Model's **interpretability** is crucial for:

- Business adoption
- Model documentation
- Regulatory oversight
- Human acceptance and trust

**Interpretable Machine Learning** provides solutions to explain at different levels.

- **Global Interpretation** explain  how the model makes predictions, based on a holistic view of its **features**. They can explain:
  - which features are the most influential (via **feature importance**)
  - How the most influential variables drive the model output (via **feature effects**)

- **Local Interpretation** becomes very important when the effect of most influential model's features doesn't the biggest influence for a particular observation as it helps us to understand what **features are influencing the predicted response** for a given observation or an small group of observations.
  - Local interpretable model-agnostic explanations (LIME)
  - Shapley values
  - Localized step-wise procedures

## Setting enviroment

### Loading libraries

```{r libraries, message=FALSE, echo=TRUE, results='hide'}
# Helper packages
library(dplyr)      # for data wrangling
library(ggplot2)    # for awesome graphics

# Modeling packages
library(h2o)       # for interfacing with H2O
library(recipes)   # for ML recipes
library(rsample)   # for data splitting
library(xgboost)   # for fitting GBMs

# Model interpretability packages
library(pdp)       # for partial dependence plots (and ICE curves)
library(vip)       # for variable importance plots
library(iml)       # for general IML-related functions
library(DALEX)     # for general IML-related functions
library(lime)      # for local interpretable model-agnostic explanations
```

### Getting the data

```{r getting-data, message=FALSE, echo=TRUE, results='hide'}
# Load and split the Ames housing data
ames <- AmesHousing::make_ames()

# for reproducibility
set.seed(123)  
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
ames_test <- testing(split)

# Make sure we have consistent categorical levels
blueprint <- recipe(Sale_Price ~ .,
                    data = ames_train) %>%
  step_other(all_nominal(),
             threshold = 0.005)

# Starting H2O
h2o.init(max_mem_size = "10g")
options(timeout = 5000)

# Create training set for h2o
train_h2o <- prep(blueprint,
                  training = ames_train,
                  retain = TRUE) %>%
  juice() %>%
  as.h2o()

# Create testing set for h2o
test_h2o <- prep(blueprint,
                 training = ames_train) %>%
  bake(new_data = ames_test) %>%
  as.h2o()

# Get response and feature names
Y <- "Sale_Price"
X <- setdiff(names(ames_train), Y)
```

### Training the model

```{r train-base-learners}
# Regularized regression base learner
best_glm <- h2o.glm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  alpha = 0.1,
  remove_collinear_columns = TRUE,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123
)

# Random forest base learner
best_rf <- h2o.randomForest(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 500,
  mtries = 20,
  max_depth = 30,
  min_rows = 100,
  sample_rate = 0.8,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)


# GBM base learner
best_gbm <- h2o.gbm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 500,
  learn_rate = 0.01,
  max_depth = 7,
  min_rows = 5,
  sample_rate = 0.8,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# XGBoost base learner
#   We cannot run this under Windows

# Stacked model
ensemble_tree <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "my_tree_ensemble",
  base_models = list(best_glm, best_rf, best_gbm),
  # Meta learner: random forest
  metalearner_algorithm = "drf"
)
```

### Defining Local Observations to Explain

```{r local_values}
# Compute predictions
predictions <- predict(ensemble_tree,
                       train_h2o) |> 
  as.vector()

max(predictions) |>
  scales::dollar() |>
  paste("Observation",
        which.max(predictions), 
        "has a predicted sale price of",
        a = _)

min(predictions) |>
  scales::dollar() |>
  paste("Observation",
        which.min(predictions), 
        "has a predicted sale price of",
        a = _) 


# Grab feature values for observations with min/max predicted sales price
high_ob <- as.data.frame(train_h2o)[which.max(predictions), ] |> 
  select(-Sale_Price)

low_ob  <- as.data.frame(train_h2o)[which.min(predictions), ] |>
  select(-Sale_Price)
```


## Interpretation trade-off

In prior chapters we have been using *model-specific* as they are closer **tied to the model performance**, but you would be facing the next limitations:

- Not all *models have a method*. For example, the *stacked model* we trained during last chapter.
- Comparing feature importance across model classes is difficult *since you are comparing different measurements*.

**There isn't a Best Approach**

> The only way to **full trust** our model interpretations is to apply multiple approaches *including model specific and model agnostic results*.


## Model-specific implementation

By reviewing the documentation of the `vip::vi_model` functions we can see some descriptions some methods work, some of the use **permutation** (*sampling without replacement*) to validate importance.

|Source package |Method Description|
|:--------|:-----------------------------|
|**lm**| It uses the absolute value of the  **$t$–statistic** as a measure of feature importance|
|**glmnet**| It uses the absolute value of the **coefficients** are returned for a specific model, so it's important to standardized the features prior fitting the model. We can specify which coefficients to return by passing the specific value of the penalty parameter via the `lambda`|
|**earth**|1. `type = "nsubsets"` counts the number of **model subsets that include each feature**. Variables that are included in more subsets are considered more important. <br> <br> 2. `type = "rss"` **calculates the decrease in the RSS for each subset** relative to the previous subset during earth()’s backward pass. Then it **sums these decreases** over all subsets that include **each variable** and **scale the result** to report 100 for the variable associated with the **larger net decrease**. <br> <br> 3. `type = "gcv"` which is a shortcut for *leave-one-out cross validation* and follow the same strategy as the *rss* with the difference that a variable might have a negative total importance as this measure no always decrease.|
|**rpart**|It records the **sum of the goodness of split** for each split and variable  plus "goodness"* for all **splits in which it was a surrogate**.|
|**randomForest**|1. `type = 1` (*mean decrease in accuracy*) **records** the prediction out-of-bag **error rate or MSE** for each tree, then repeats the process after **permuting each predictor** variable, calculate the *difference between both errors*, takes the average over all trees and normalized the results using the standard deviation of the differences. <br> <br> 2. `type = 2` (*mean decrease in node impurity*) is the total decrease in node impurities (**Gini index** or **residual sum of squares**) from splitting on the variable, averaged over all trees.|
|**gbm**|1. If `type = "relative.influence"` and `distribution = "gaussian"` this returns the reduction of squared error attributable to each variable. <br> <br> 2. If  `distribution != "gaussian"` returns the reduction attributable to each variable in **sum of squared error** in predicting the gradient on **each iteration** to report the relative influence of each variable in **reducing the loss function**. <br> <br> 3. If `type = "permutation"` it **randomly permutes each predictor variable** at a time and computes the associated reduction in **predictive performance** using using the **entire training dataset**.|
|**xgboost**|1. For linear models, the importance is the absolute magnitude of linear coefficients. <br> <br> 2. For tree models and `type = "gain"` it gives the **fractional contribution of each feature** to the model based on the total gain (*sum of the improvements in accuracy brought by that feature to all the branches it is on.*) of the corresponding feature's splits. <br> <br> 3. For tree models and `type = "cover"` it gives the number of observations related to each feature. <br> <br> 4. For tree models and `type = "frequency"` it gives the percentages representing the relative number of times each feature has been used throughout each tree in the ensemble.

## Permutation-based feature importance

### Concept

We can use this method for any model after assuring that:

- **The model isn't overfitting**: To avoid interpreting *noise* rather than *signal*.
- **Features present low correlation**: To avoid under estimating the importance of correlated features, as permuting a feature without permuting its correlated pairs won't have a big effect.

It measures feature’s importance by calculating the **increase of the model’s prediction error**, by taking the *difference to interpret absolute values* or the *ratio to interpret relative values* of a metric like the *RMSE* after **permuting the feature**, as describes the next *algorithm*:

```
For any given loss function do the following:
1. Compute loss function for original model
2. For variable i in {1,...,p} do
     | randomize values
     | apply given ML model
     | estimate loss function
     | compute feature importance (some difference/ratio measure 
       between permuted loss & original loss)
   End
3. Sort variables by descending feature importance
```

![Source: https://www.aporia.com/learn/feature-importance/explain-ml-models-with-permutation-importance/](chapter-img/16_chapter/01-permutation.webp)

### Implementation

**General Description**

|**Package**|**Function**|**Description**|
|:----------|:-----------|:--------------|
|**iml**|`FeatureImp()`|- Several loss functions (`loss = "mse"`) <br> - Difference and ratio calculations.|
|**DALEX**|`variable_importance()`|- Several loss functions. <br> - Difference and ratio calculations. <br> - It can sample the data (`n_sample = 1000`)|
|**vip**|`vip()` or `vi_permute()`|- Supports model-specific and model-agnostic approaches. <br> - Custom and several loss functions. <br> - Perform a Monte Carlo simulation to stabilize the procedure. (`nsim = 5`) <br> - It can sample the data (`sample_size` or `sample_frac`) <br> - It can perform parallel computations (`parallel = TRUE`)|

**Creating Needed Objects**

```{r support-objects}
# For iml and DALEX
# 1) Split the features and response
features <- as.data.frame(train_h2o) |> select(-Sale_Price)
response <- as.data.frame(train_h2o) |> pull(Sale_Price)

# 2) Create custom predict function that returns the predicted values as a vector
pred <- function(object, newdata)  {
  results <- as.vector(h2o.predict(object, as.h2o(newdata)))
  return(results)
}

## For DALEX
components_dalex <- DALEX::explain(
  model = ensemble_tree,
  data = features,
  y = response,
  predict_function = pred
) 
```

**iml**

```{r iml-permutation}
Predictor$new(ensemble_tree, data = features, y = response) |>
  FeatureImp$new(loss = "rmse", compare = "difference") |>
  plot()
```


**DALEX**

```{r DALEX-permutation}
loss_default(components_dalex$model_info$type)

model_parts(components_dalex,
            type = "difference") |>
  plot()
```


**vip**

```{r vip-permutation}
vip(
  ensemble_tree,
  train = as.data.frame(train_h2o),
  method = "permute",
  target = "Sale_Price",
  metric = "RMSE",
  type = "difference",
  nsim = 5,
  sample_frac = 0.5,
  pred_wrapper = pred
)
```

![](chapter-img/16_chapter/03-vip-permutation.png)

## Partial dependence

### Concept

It allows us to understand **how the response variable changes** as we **change the value of a feature** while taking into account the average effect of all the other features in the model, when the model has **weak interactions between features**.

1. Splitting the feature of interest into  $j$ equally spaced values. For example, if $j = 20$ for the `Gr_Liv_Area` we could write the next code.

```{r j_values, eval=TRUE}
j_values <-
  seq(from = 334,
      to = 5095,
      length.out = 20L) |>
  round(digits = 2)

matrix(j_values, nrow = 5, byrow = TRUE)
```

2. Making $j$ copies of the original training data.

3. For each copy setting the `Gr_Liv_Area` with the corresponding value. For example, the value for first copy will be `r j_values[1L]` and the second `r j_values[2L]` until changing the last copy `r tail(j_values, 1L)`.

4. **Predicting the outcome** for each observation in each of the $j$ copies.

5. **Averaging the predicted values** for each set.

6. **Plotting** the average of each $j$ value.

![](chapter-img/16_chapter/02-pdp-illustration.png)


### Implementation

**DALEX**

```{r DALEX-pdp}
dalex_pdp <- model_profile(explainer = components_dalex,
                           variables = "Gr_Liv_Area",
                           N = 20L)

dalex_pdp

plot(dalex_pdp)
```

**pdp**

```{r pdp-pdp}
# Custom prediction function wrapper
pdp_pred <- function(object, newdata)  {
  results <- mean(as.vector(h2o.predict(object, as.h2o(newdata))))
  return(results)
}

# Compute partial dependence values
pd_values <- partial(
  ensemble_tree,
  train = as.data.frame(train_h2o), 
  pred.var = "Gr_Liv_Area",
  pred.fun = pdp_pred,
  grid.resolution = 20
)

head(pd_values)
##   Gr_Liv_Area     yhat
## 1         334 158858.2
## 2         584 159566.6
## 3         835 160878.2
## 4        1085 165896.7
## 5        1336 171665.9
## 6        1586 180505.1

autoplot(pd_values,
         rug = TRUE,
         train = as.data.frame(train_h2o))
```

![](chapter-img/16_chapter/04-pdp-plot.png)

### Adding feature importance

We can assume that those features with **larger marginal effects** on the response have greater importance, this is really useful to create plots of both feature importance and feature effects.

```{r vip-pdp}
pd_imp_values <- vip(
  ensemble_tree,
  train = as.data.frame(train_h2o),
  method = "firm",
  feature_names = "Gr_Liv_Area",
  pred.fun = pdp_pred,
  grid.resolution = 20,
  ice = FALSE
)

head(pd_imp_values)
```

## Individual conditional expectation

### Concept

ICE plots can help to **highlight interactions between features** by showing the predicted response of each instance separately.

**Centering the ICE** curves at the *minimum value of the feature* makes easier to see:

- How effects change as the feature value increases (highlight heterogeneity)
- Observations that deviate from the general pattern.

![](chapter-img/16_chapter/05-ice-illustration.png)

### Implementation

```{r}
# Construct c-ICE curves
partial(
  ensemble_tree,
  train = as.data.frame(train_h2o), 
  pred.var = "Gr_Liv_Area",
  pred.fun = pred,
  grid.resolution = 20,
  plot = TRUE,
  center = TRUE,
  plot.engine = "ggplot2"
)
```

![](chapter-img/16_chapter/06-Centered-ICE-curve-for-Gr_Liv_Area.png)


> **PDPs**, **ICE curves** and **c-ICE curves** for classification models are typically plotted on a **logit-type scale** to facilitate interpretation.

```{r}
h2o::h2o.shutdown(prompt = FALSE)
```


## Meeting Videos {-}

### Cohort 1 {-}

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>
<summary> Meeting chat log </summary>

```
LOG
```
</details>
