# Stacked Models

**Learning objectives:**

- THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY

## Base idea {-}

- Imagine you have a group of *individual learners* who are trying to solve a problem. Each learner has its own strengths and weaknesses, and they may not agree on the best solution. How can you combine their opinions and get the most out of them? This is the idea behind **stacking**.

![](chapter-img/15_chapter/01-2-robots.jfif)

- **Stacking** is a technique that uses a *combiner* to merge the predictions of several *individual learners* or **base learners**. The *combiner* is also known as **meta algorithm** or **super learner**, because it learns from the other learners and produces a superior prediction.

![](chapter-img/15_chapter/02-stack-models.jfif)

- This technique was introduced by Leo Breiman in **1996**, but it was considered as a **black art** until **2007**, when Van der Laan, Polley, and Hubbard developed its theoretical background. They showed that stacking **can achieve optimal performance** when base learners present **high variability** and **uncorrelated predicted** values. 

under certain conditions, and they provided practical guidelines for its implementation.

![](chapter-img/15_chapter/03-paper-doubts.jfif)

## Setting enviroment {-}

**Loading libraries**

```{r libraries, message=FALSE, echo=TRUE, results='hide'}
# Helper packages
#  For creating our train-test splits
library(rsample)
#  For minor feature engineering tasks
library(recipes)

# Modeling packages
library(h2o)
h2o.init(max_mem_size = "13g")
```

**Getting our training set**

```{r getting-data, message=FALSE}
# Load and split the Ames housing data
ames <- AmesHousing::make_ames()

# for reproducibility
set.seed(123)  
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
ames_test <- testing(split)
```

**Loading data to h2o session**

```{r h2o_data, message=FALSE, echo=TRUE, results='hide'}
# Make sure we have consistent categorical levels
blueprint <- recipe(Sale_Price ~ .,
                    data = ames_train) %>%
  step_other(all_nominal(),
             threshold = 0.005)

# Create training set for h2o
train_h2o <- prep(blueprint,
                  training = ames_train,
                  retain = TRUE) %>%
  juice() %>%
  as.h2o()

# Create testing set for h2o
test_h2o <- prep(blueprint,
                 training = ames_train) %>%
  bake(new_data = ames_test) %>%
  as.h2o()

# Get response and feature names
Y <- "Sale_Price"
X <- setdiff(names(ames_train), Y)
```

## Process Description

1. Set up the ensemble by
  - Defining a list $L$ of tuned based learners
  - Defining a meta learner algorithm (usually *some form of regularized regression*)
  
2. Train the ensemble by
  - Training each base learner
  - Performing **k-fold CV** on each of the base learners and collect the cross-validated predictions from each, to avoid **overfitting** as they would be predicting **new data**.
  - Creating the $Z$ feature matrix of $N \times L$ known as "level-one"
  - Training the meta learning algorithm on the level-one data $y = f(Z)$
  
3. To make predicts
  - Generate predictions from each base learner
  - Feed those predictions into the meta learner to generate the ensemble prediction
  

## R packages available

| Package | Description | Availability | Algorithm | Parallelization |
|---------|-------------|--------------|-----------|-----------------|
| stacks  | An R package for model stacking that aligns with the tidymodels. It uses a regularized linear model to combine predictions from ensemble members. | CRAN and GitHub | Super Learner | No |
| SuperLearner | An R package that provides the original Super Learner and includes a clean interface to 30+ algorithms. | CRAN and GitHub | Super Learner | No |
| subsemble | An R package that provides stacking via the super learner algorithm and also implements the subsemble algorithm. | GitHub only | Super Learner and Subsemble | Yes |
| caretEnsemble | An R package that provides an approach for stacking, but it implements a bootstrapped version of stacking. | CRAN and GitHub | Bootstrap Stacking | No |
| h2o    | An R package that provides an efficient implementation of stacking and allows you to stack existing base learners, stack a grid search, and also implements an automated machine learning search with stacked results. | CRAN and GitHub | Stacked Ensembles and AutoML | Yes |

## Training example

1. Train each base model using the same seed (`seed = 123`), fold assignment (`fold_assignment = "Modulo"`) and saving the cross-validated predictions (`keep_cross_validation_predictions = TRUE`).

```{r}
# Regularized regression base learner
best_glm <- h2o.glm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  alpha = 0.1,
  remove_collinear_columns = TRUE,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123
)

# Random forest base learner
best_rf <- h2o.randomForest(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 1000,
  mtries = 20,
  max_depth = 30,
  min_rows = 1,
  sample_rate = 0.8,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)


# GBM base learner
best_gbm <- h2o.gbm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.01,
  max_depth = 7,
  min_rows = 5,
  sample_rate = 0.8,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# XGBoost base learner
best_xgb <- h2o.xgboost(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.05,
  max_depth = 3,
  min_rows = 3,
  sample_rate = 0.8,
  categorical_encoding = "Enum",
  nfolds = 10,
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE,
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)
```


2. Set up the ensemble by
  - Defining a list $L$ of tuned based learners
  - Defining a metalearner algorithm

```{r}
# Stacked model
ensemble_tree <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "my_tree_ensemble",
  base_models = list(best_glm, best_rf, best_gbm, best_xgb),
  # Meta learner: random forest
  metalearner_algorithm = "drf"
)
```

3. Comparing model performance

```{r}
# Get results from base learners
get_rmse <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$RMSE
}

list(glm = best_glm,
     rf = best_rf,
     gbm = best_gbm,
     xgb = best_xgb) %>%
  sapply(get_rmse)

# Stacked results
get_rmse(ensemble_tree)
```


```{r end_h2o, message=FALSE, echo=TRUE, results='hide'}
h2o.shutdown(prompt = FALSE)
```


## Meeting Videos {-}

### Cohort 1 {-}

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>
<summary> Meeting chat log </summary>

```
LOG
```
</details>
